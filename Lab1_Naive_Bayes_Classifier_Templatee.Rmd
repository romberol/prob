---
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

### *Name1 Surname1, Name2 Surname2, Name3 Surname3*

## Introduction

During the past three weeks, you learned a couple of essential notions
and theorems, and one of the most important among them is the *Bayes
theorem*.

One of its applications is **Naive Bayes classifier**, which is a
probabilistic classifier whose aim is to determine which class some
observation probably belongs to by using the Bayes formula:
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

Under the strong independence assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given
observation. Thus, $\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now
can be calculated as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}\tag{1}$$

All the terms on the right-hand side can be estimated from the data as
respective relative frequencies;\
see [this
site](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/)
for more detailed explanations.

## Data description

There are 5 datasets uploaded on the cms.

To determine your variant, take your team number from the list of teams
on cms and take *mod 5* - this is the number of your data set.

-   **0 - authors** This data set consists of citations of three famous
    writers: Edgar Alan Poe, Mary Wollstonecraft Shelley and HP
    Lovecraft. The task with this data set is to classify a piece of
    text with the author who was more likely to write it.

-   **1 - discrimination** This data set consists of tweets that have
    discriminatory (sexism or racism) messages or of tweets that are of
    neutral mood. The task is to determine whether a given tweet has
    discriminatory mood or does not.

-   **2 - fake news** This data set contains data of American news: a
    headline and an abstract of the article. Each piece of news is
    classified as fake or credible. The task is to classify the news
    from test.csv as credible or fake.

-   **3 - sentiment** All the text messages contained in this data set
    are labeled with three sentiments: positive, neutral or negative.
    The task is to classify some text message as the one of positive
    mood, negative or neutral.

-   **4 - spam** This last data set contains SMS messages classified as
    spam or non-spam (ham in the data set). The task is to determine
    whether a given message is spam or non-spam.

Each data set consists of two files: *train.csv* and *test.csv*. The
first one you will need find the probabilities distributions for each of
the features, while the second one is needed for checking how well your
classifier works.

```{r}
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(tidyr)
library(readr)
library(dplyr)
library(ggplot2)
library(caret)
library(yardstick)
```

## Instructions

-   The first step is data pre-processing, which includes removing
    punctuation marks and stop words

-   represent each message as a bag-of-words

-   using the training set, calculate all the conditional probabilities
    in formula (1)

-   use those to predict classes for messages in the test set

-   evaluate effectiveness of the classifier by calculating the
    corresponding metrics

-   shortly summarize your work

-   do not forget to submit both the (compiled) Rmd source file and the .html
    output
    
### Data pre-processing

-   Read the *.csv* data files.
-   Ð¡lear your data from punctuation or other unneeded symbols.
-   Clear you data from stop words. You don't want words as is, and, or
    etc. to affect your probabilities distributions, so it is a wise
    decision to get rid of them. Find list of stop words in the cms
    under the lab task.
-   Represent each test message as its bag-of-words. Here:
    <https://machinelearningmastery.com/gentle-introduction-bag-words-model/>
    you can find general introduction to the bag-of-words model and
    examples on to create it.
-   It is highly recommended to get familiar with R dataframes, it would
    make the work much easier to do.
-   Useful links:
    -   <https://steviep42.github.io/webscraping/book/bagofwords.html#tidytext> -
        example of using *tidytext* to count frequencies of the words.
    -   Basics of Text Mining in R:
        <http://rstudio-pubs-static.s3.amazonaws.com/256588_57b585da6c054349825cba46685d8464.html>
        . Note that it also includes an example on how to create a bag
        of words from your text document.

```{r}
list.files(getwd())
list.files("data/0-authors")
```

```{r}
test_path <- "data/0-authors/test.csv"
train_path <- "data/0-authors/train.csv"

stop_words_txt <- read_file("stop_words.txt")
# https://stackoverflow.com/questions/27195912/why-does-strsplit-return-a-list
splitted_stop_words <- strsplit(stop_words_txt, split='\n')
splitted_stop_words <- splitted_stop_words[[1]]

```

```{r}
train <-  read.csv(file = train_path, stringsAsFactors = FALSE)
test <-  read.csv(file = test_path, stringsAsFactors = FALSE)
```

```{r}
# note the power functional features of R bring us! 
tidy_text <- unnest_tokens(train, 'splitted', 'text', token="words") %>%
             filter(!splitted %in% splitted_stop_words)

words <- tidy_text %>%
  count(splitted,sort=TRUE, author) %>%
  # pivot_wider(names_from = splitted, values_from = n, names_repair="minimal")
  pivot_wider(names_from = author, values_from = n)

#test
words[is.na(words)] <- 0; # replaces na with 0
words[, 2:length(words)] <- words[, 2:length(words)]+1 # add ones
words <- data.frame(words[, -1], row.names = words$splitted) #words are indexes
# words[[1, 2]] / sum(words[, 2])
# count(train, author)
head(words)
```
```{r}
#visualization of top-used words among authors
library(ggplot2)
library(reshape2)

for(i in 1:ncol(words)){
  #View(words[i])
  #words1 <- words[order(words[i], decreasing = TRUE),]
  #words[order(words[,3], decreasing = TRUE),]
  top10 <- head(words[order(words[,i], decreasing = TRUE), ], 10)
  top10<- cbind(rownames(top10), top10)
  rownames(top10)<-NULL
  colnames(top10) <- c("word", "Edgar.Alan.Poe", "HP.Lovecraft", "Mary.Wollstonecraft.Shelley.")
  dfi <- melt(top10, "word")
  print(dfi%>%ggplot(aes(x=word, y=value)) +
  geom_bar(aes(fill=variable),stat="identity", position ="dodge") + 
  theme_bw()+ 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)))
}

```


### Data visualization

Each time you work with some data, you need to understand it before you
start processing it. R has very powerful tools to make nice plots and
visualization. Show what are the most common words for negative and
positive examples as a histogram, word cloud etc. Be creative!
```{r}
X <- train[, -length(train)]
y <- train[length(train)]
X_test <- test[, -length(test)]
y_test <- test[length(test)]
```


## Classifier implementation

```{r}
naiveBayes <- setRefClass("naiveBayes",
                          
       # here it would be wise to have some vars to store intermediate result
       # frequency dict etc. Though pay attention to bag of wards!
       fields = list(
         data = "data.frame",
         labels = "table"
       ),
       methods = list(
                    # prepare your training data as X - bag of words for each of your
                    # messages and corresponding label for the message encoded as 0 or 1 
                    # (binary classification task)
                    fit = function(X, y)
                    {
                        labels <<- table(y) / sum(table(y))
                        X <- cbind(X, y)
                        tidy_text <- unnest_tokens(X, 'splitted', 'text', token="words") %>%
                          filter(!splitted %in% splitted_stop_words)

                        words <- tidy_text %>%
                          count(splitted,sort=TRUE, author) %>%
                          # pivot_wider(names_from = splitted, values_from = n, names_repair="minimal")
                          pivot_wider(names_from = author, values_from = n)

                        #test
                        words[is.na(words)] <- 0; # replaces na with 0
                        words[, 2:length(words)] <- words[, 2:length(words)]+1 # add ones
                        words <- data.frame(words[, -1], row.names = words$splitted)
                        for(i in 1:3){
                          words[i] <- words[i]/sum(words[i])
                        }
                        data <<- words
                    },
                    
                    # return prediction for a single message 
                    predict = function(message)
                    {
                      message <- gsub("[^'[:^punct:]]", "", tolower(message), perl = T)
                      splited_words <- strsplit(message, split=" ")[[1]]
                      results <- seq_along(labels)
                      # match(max(a), a) - argmax
                      for (i in results){
                        prob <- 1
                        prob <- prob *labels[[i]]
                        # prob <- prob * labels[[i]]
                        for (word_ind in seq_along(splited_words)){
                          if (!splited_words[word_ind] %in% splitted_stop_words){
                            # prob <- prob * data[splited_words[word_ind], ][[i]] / sum(data[i])
                            if (!is.na(data[splited_words[word_ind], ][[i]])){
                                prob <- prob * data[splited_words[word_ind], ][[i]]
                            }
                          }
                        }
                        results[i] <- prob
                      }

                      return(names(labels)[match(max(results), results)])
                      # return(results)
                    },
                    
                    # score you test set so to get the understanding how well you model
                    # works.
                    # look at f1 score or precision and recall
                    # visualize them 
                    # try how well your model generalizes to real world data! 
                    score = function(X_test, y_test){
                      correct <- 0
                      for(i in seq_along(X_test$text)){
                        if (predict(X_test$text[i])==y_test$author[i]){
                          correct <- correct + 1
                        }
                      }
                      return(correct / length(X_test$text))
                    },

                    list_of_predictions = function(X_test, y_test)
                    #   Returns dataframe, where first column is labels, second is predicted values
                    #   Used to calculate confusion matrix
                    {
                      result <- data.frame(matrix(0, nrow=length(X_test$text), ncol=1))
                      colnames(result) <- c("predicted")
                      for (i in 1:3){
                        y_test$author[y_test$author == names(labels)[i]] <- i
                      }
                      result["predicted"] <- apply(X_test["text"], 1, FUN=.self$predict)
                      y_test$author <- as.factor(y_test$author)
                      result$predicted <- as.factor(result$predicted)
                      # return(confusionMatrix(y_test$author, result$predicted))
                      return(cbind(y_test, result))
                    }
))

model <- naiveBayes()
model$fit(X, y)
# b <- model$predict("ten or twenty maouths or trunks a stickin' aout all along the sides, big as stovepipes, an' all a tossin' an' openin' an' shuttin' . . .")
score <- model$list_of_predictions(X_test, y_test)
```

```{r}
conf_mx <- confusionMatrix(score$author, score$predicted)
print(conf_mx$overall[1])
print("Precision:")
print(conf_mx$byClass[,"Precision"])
print("Recall:")
print(conf_mx$byClass[,"Recall"])
print("F1:")
print(conf_mx$byClass[,"F1"])
```

```{r}
autoplot(conf_mat(score, author, predicted), type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1") +
  theme(legend.position = "right")
```

```{r}
#using built-in stop-words
naiveBayes2 <- setRefClass("naiveBayes2",
                          
       # here it would be wise to have some vars to store intermediate result
       # frequency dict etc. Though pay attention to bag of wards!
       fields = list(
         data1 = "data.frame",
         labels = "table"
       ),
       methods = list(
                    # prepare your training data as X - bag of words for each of your
                    # messages and corresponding label for the message encoded as 0 or 1 
                    # (binary classification task)
                    fit = function(X, y)
                    {
                        labels <<- table(y) / sum(table(y))
                        X <- cbind(X, y)
                        names(stop_words)[names(stop_words) == 'word'] <- 'splitted'
                        tidy_text <- unnest_tokens(X, 'splitted', 'text', token="words") %>%
                          anti_join(stop_words)  #get rid of stop_words using built-in df and function anti_join

                        words <- tidy_text %>%
                          count(splitted,sort=TRUE, author) %>%
                          # pivot_wider(names_from = splitted, values_from = n, names_repair="minimal")
                          pivot_wider(names_from = author, values_from = n)

                        #test
                        words[is.na(words)] <- 0; # replaces na with 0
                        words[, 2:length(words)] <- words[, 2:length(words)]+1 # add ones
                        #words <- data.frame(words[, -1], row.names = words$splitted)
                        words <- words %>% mutate_at(vars(2:4), funs(./sum(.)))
                        #View(words)
                        #for(i in 1:3){
                         # words[i] <- words[i]/sum(words[i])
                        #}
                        data1 <<- words
                    },
                    
                    # return prediction for a single message 
                    predict = function(message)
                    {
                      msg_df = data.frame(message)
                      msg_df <- unnest_tokens(msg_df, 'word', 'message',
                                              token="words")%>%anti_join(stop_words)
                      #message <- gsub("[^'[:^punct:]]", "", tolower(message), perl = T)
                      #splited_words <- strsplit(message, split=" ")[[1]]
                      #msg_df <- data.frame(splited_words)
                      #colnames(msg_df)<-c('splitted')
                      #print(msg_df$splitted)
                      #names(stop_words)[names(stop_words) == 'word'] <- 'splitted'
                      #View(msg_df)
                      #View(stop_words)
                      #msg_df%>% anti_join(stop_words, by=c("splitted"="word"))
                      
                      results <- seq_along(labels)
                      # match(max(a), a) - argmax
                      #data1 <- cbind(rownames(data1), data1)
                       # rownames(data1) <- NULL
                        #colnames(data1) <- c("word","Mary.Wollstonecraft.Shelley.",
                         #                "Edgar.Alan.Poe", "HP.Lovecraft"
                      
                      
                      for (i in results){
                        prob <- 1
                        prob <- prob *labels[[i]]
                        # prob <- prob * labels[[i]]
                        #names(msg_df)[names(msg_df) == 'slpitted'] <- i
                        #View(msg_df)
                        
                        
                        msg_df <- msg_df%>%inner_join(data1,by=c("word"="splitted"))
                        
                        p <- apply(msg_df[2], 2, prod)
                        prob <- prob*p
                        #for (word_ind in seq_along(splited_words)){
                         # if (!splited_words[word_ind] %in% splitted_stop_words){
                            # prob <- prob * data[splited_words[word_ind], ][[i]] / sum(data[i])
                        #    if (!is.na(data[splited_words[word_ind], ][[i]])){
                         #       prob <- prob * data[splited_words[word_ind], ][[i]]
                          #  }
                        #  }
                        #}
                        results[i] <- prob
                      }

                      return(names(labels)[match(max(results), results)])
                      # return(results)
                    },
                    
                    # score you test set so to get the understanding how well you model
                    # works.
                    # look at f1 score or precision and recall
                    # visualize them 
                    # try how well your model generalizes to real world data! 
                    score = function(X_test, y_test) #
                    {
                      correct <- 0
                      for(i in seq_along(X_test$text)){
                        if (predict(X_test$text[i])==y_test$author[i]){
                          correct <- correct + 1
                        }
                      }
                      return(correct / length(X_test$text))

                    },
                    list_of_predictions = function(X_test, y_test)
                    #   Returns dataframe, where first column is labels, second is predicted values
                    #   Used to calculate confusion matrix
                    {
                      result <- data.frame(matrix(0, nrow=length(X_test$text), ncol=1))
                      colnames(result) <- c("predicted")
                      for (i in 1:3){
                        y_test$author[y_test$author == names(labels)[i]] <- i
                      }
                      result["predicted"] <- apply(X_test["text"], 1, FUN=.self$predict)
                      y_test$author <- as.factor(y_test$author)
                      result$predicted <- as.factor(result$predicted)
                      # return(confusionMatrix(y_test$author, result$predicted))
                      return(cbind(y_test, result))
                    }
))

model <- naiveBayes2()
model$fit(X, y)
# b <- model$predict("ten or twenty maouths or trunks a stickin' aout all along the sides, big as stovepipes, an' all a tossin' an' openin' an' shuttin' . . .")
b <- model$list_of_predictions(X_test, y_test)
b

```
```{r}
msg <- "Hello, my name is Matthew. It's nice to meet you"
msg <- gsub("[^'[:^punct:]]", "", tolower(msg), perl = T)
splt_msg <- strsplit(msg, split=" ")[[1]]
msg_df <- data.frame(splt_msg)
msg_df
stop_words
names(msg_df)[names(msg_df) == 'splt_msg'] <- 'word'
msg_df
names(stop_words)[names(stop_words) == 'word'] <- 'splitted'
anti_join(msg_df, stop_words)
```


## Measure effectiveness of your classifier
-   Note that accuracy is not always a good metric for your classifier.
    Look at precision and recall curves, F1 score metric.
-   Visualize them.
-   Show failure cases.

## Conclusions

Summarize your work by explaining in a few sentences the points listed
below.

-   Describe the method implemented in general. Show what are
    mathematical foundations you are basing your solution on.
-   List pros and cons of the method. This should include the
    limitations of your method, all the assumption you make about the
    nature of your data etc.
